= Demo for sending Confluent Platform logs to ELK

This playground for experimenting with migrating Kafka from Zookeeper to KRaft.
It features an optional ELK (Elastic, Kibana, fleet agent) stack for easier observability.

== Preconditions

Adapt the testbench to your version of Confluent Platform as required by modifying `.env`.

== Running Apache Kafka (using the Bitnami image)

Optionally, if you want to use elastic search for log aggregation, add the "elastic" profile in addition to the profiles used in the following steps, for example:

```bash
export COMPOSE_PROFILES="zookeeper,elastic"
```
in the first phase or
```bash
export COMPOSE_PROFILES="kraft,elastic"
```
when KRaft mode is being enabled

=== Phase 1: Start a new cluster in Zookeeper mode

First, activate the docker profile zookeeper for the following commands, by setting the appropriate environment variable:

```bash
export COMPOSE_PROFILES="zookeeper"
```

Start the containers by running:
```bash
docker compose up -d
```

Wait until all container have initialized. You can check them with this command (Look at the STATUS column)
```bash
docker compose ps
```

==== Create topic and produce some data

Create a topic:

```
docker compose exec broker1 kafka-topics.sh --bootstrap-server broker1:9092 --create --topic test
```

List topics:

```
docker compose exec broker1 kafka-topics.sh --bootstrap-server broker1:9092 --list
```

Produce some test data (exit with Ctrl-D):

```
docker compose exec broker1 kafka-console-producer.sh --bootstrap-server broker1:9092 --topic test
```

Read the data again (exit with Ctrl-C):

```
docker compose exec broker1 kafka-console-consumer.sh --bootstrap-server broker1:9092 --topic test --from-beginning
```

=== Phase 2: Start the migration

First, we need to find the `Cluster ID` of the Zookeeper-based Kafka cluster. This can be found in Zookeeper by running

```bash
docker compose exec zookeeper1 zookeeper-shell localhost:21811 get /cluster/id
```

Use the string found in the `id` field of the result.

Now prepare the Docker environment for spinning up the new KRaft controllers with that ID.
In the `.env` file found in the folder of the `compose.yaml`, set the "KRAFT_CLUSTER_ID" variable to the valued of the `id` field.

Next, let's choose a docker profile where both Zookeeper and KRaft controllers are active:

```bash
export COMPOSE_PROFILES="migration"
docker compose up -d
```

Have a look at the log files of the KRaft controllers (One of `controller1`, `controller2`, or `controller3` will become the quorum controller, check all):

```bash
docker compose logs controller1
docker compose logs controller2
docker compose logs controller3
```

The controllers will initialize and wait for connections from brokers before they start the actual migration.

Update the brokers configuration.
Make sure, `CONTROLLER:PLAINTEXT` is configured in the listener security map (`KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP`), for every broker. In this demo this has been pre-configured already.



## Useful commands
Stopping the containers without removing any volumes:
```bash
docker compose  down
```

Stopping the containers with removal of all created volumes (be careful!):
```bash
docker compose  down -v
```

Cleaning up (CAREFUL: THIS WILL DELETE ALL UNUSED VOLUMES):
```bash
docker volumes prune
```

== Usage

=== ELK with Kibana (optional)

If you have used the profile `elastic`, you can access `kibana` with your web browser here:

* URL: `http://localhost:5601`
* Username: `elastic`
* Password: `elastic`

Go to `Analytics->Discover`.

If using the `filebeat` profile, create a new data view. Use any name (e.g. `kafka`) and filter on `filebeat-*`.

Choose a time interval and export data as CSV file by clicking "Share".
